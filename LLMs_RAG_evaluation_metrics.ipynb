{"cells":[{"cell_type":"markdown","metadata":{"id":"MphHdQ2o-IoK"},"source":["# **Explore LLM Evaluation Techniques**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"_E1-Owlk6-4v","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["pip install nltk rouge-score bert-score"]},{"cell_type":"markdown","metadata":{"id":"w8R7w-yp-dM6"},"source":["# **1- Automated Metrics**\n","## For Answer Quality (text generation):\n","## **BERTscore, ROUGE, and BLEU**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2uDFeYln8YB0"},"outputs":[],"source":["from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from rouge_score import rouge_scorer\n","import bert_score\n","\n","# Basic word splitter (avoids nltk.word_tokenize)\n","def tokenize(text):\n","    return text.lower().split()\n","\n","def compute_bleu(reference, candidate):\n","    ref_tokens = [tokenize(reference)]\n","    cand_tokens = tokenize(candidate)\n","    smoothie = SmoothingFunction().method4\n","    return sentence_bleu(ref_tokens, cand_tokens, smoothing_function=smoothie)\n","\n","def compute_rouge(reference, candidate):\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","    scores = scorer.score(reference, candidate)\n","    return scores\n","\n","def compute_bert_score(references, candidates):\n","    P, R, F1 = bert_score.score(candidates, references, lang=\"en\", verbose=False)\n","    return {\n","        \"precision\": P.mean().item(),\n","        \"recall\": R.mean().item(),\n","        \"f1\": F1.mean().item()\n","    }\n","\n","if __name__ == \"__main__\":\n","    reference = \"The capital of France is Paris.\"\n","    candidate = \"Paris is the capital of France.\"\n","\n","    print(\"BLEU Score:\", compute_bleu(reference, candidate))\n","\n","    rouge_scores = compute_rouge(reference, candidate)\n","    print(\"ROUGE Scores:\", {k: round(v.fmeasure, 4) for k, v in rouge_scores.items()})\n","\n","    bert_scores = compute_bert_score([reference], [candidate])\n","    print(\"BERTScore:\", {k: round(v, 4) for k, v in bert_scores.items()})\n"]},{"cell_type":"markdown","metadata":{"id":"toCJiiThAEZF"},"source":["# **2- LLM As A Judge \"G-Eval\"**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"nsNvZ0XkBX1H","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["!pip install openai"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"niqfWJH3Dino"},"outputs":[],"source":["\n","def judge_with_openai(question, candidate, reference):\n","    prompt = f\"\"\"\n","Evaluate this answer to a question using 1â€“5 ratings:\n","\n","Question: {question}\n","Candidate Answer: {candidate}\n","Reference Answer: {reference}\n","\n","Give ratings (1 to 5) for:\n","- Relevance\n","- Faithfulness\n","- Correctness\n","\n","Return JSON like: {{\"relevance\": 4, \"faithfulness\": 5, \"correctness\": 4}}\n","\"\"\"\n","\n","    response = client.chat.completions.create(\n","        model=\"gpt-4\",\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"You are an expert grader for short answers.\"},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ],\n","        temperature=0,\n","        max_tokens=150\n","    )\n","\n","    result = response.choices[0].message.content.strip()\n","    print(result)\n","\n","# Example usage\n","question = \"What is the capital of France?\"\n","candidate = \"Paris is the capital of France.\"\n","reference = \"The capital of France is Paris.\"\n","\n","judge_with_openai(question, candidate, reference)\n"]},{"cell_type":"markdown","metadata":{"id":"8-Hbxo4zE6pZ"},"source":["# **3-Retrieval Component Metrics for RAG**\n"]},{"cell_type":"markdown","metadata":{"id":"a65hGy6BFUSI"},"source":["**Precision@k**: Measures the proportion of relevant documents in the top k retrieved documents.\n","\n","**Recall@k**: Measures the proportion of relevant documents retrieved in the top k out of all relevant documents."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HT_ZNm2eFyEL"},"outputs":[],"source":["import numpy as np\n","\n","def precision_at_k(retrieved_docs, relevant_docs, k):\n","    retrieved_at_k = np.array(retrieved_docs[:k])\n","    relevant_at_k = np.array(relevant_docs[:k])\n","    true_positives = np.sum((retrieved_at_k == 1) & (relevant_at_k == 1))\n","    return true_positives / k\n","\n","def recall_at_k(retrieved_docs, relevant_docs, k):\n","    retrieved_at_k = np.array(retrieved_docs[:k])\n","    relevant_at_k = np.array(relevant_docs[:k])\n","    true_positives = np.sum((retrieved_at_k == 1) & (relevant_at_k == 1))\n","    total_relevant = np.sum(np.array(relevant_docs) == 1)  # use all, not just top-k\n","    return true_positives / total_relevant if total_relevant > 0 else 0\n","\n","# Example usage\n","relevant_docs = [1, 0, 1, 0, 1]\n","retrieved_docs = [1, 1, 0, 1, 0]\n","\n","precision_k3 = precision_at_k(retrieved_docs, relevant_docs, 3)\n","recall_k3 = recall_at_k(retrieved_docs, relevant_docs, 3)\n","\n","print(f\"Precision at k=3: {precision_k3:.2f}\")\n","print(f\"Recall at k=3: {recall_k3:.2f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"hZ2wwVUtF-aW"},"source":["# **4- Benchmark Datasets for RAG and LLM Evaluation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJmQVXfVGxaB"},"outputs":[],"source":["# MS MARCO (Microsoft MAchine Reading COmprehension):\n","# Contains a large set of questions and answers to evaluate retrieval-based models.\n","\n","def precision_at_k(retrieved_docs, relevant_docs, k):\n","    retrieved_k = retrieved_docs[:k]\n","    relevant_set = set(relevant_docs)\n","    true_positives = sum(1 for doc in retrieved_k if doc in relevant_set)\n","    return true_positives / k\n","\n","def recall_at_k(retrieved_docs, relevant_docs, k):\n","    retrieved_k = retrieved_docs[:k]\n","    relevant_set = set(relevant_docs)\n","    true_positives = sum(1 for doc in retrieved_k if doc in relevant_set)\n","    total_relevant = len(relevant_set)\n","    return true_positives / total_relevant if total_relevant > 0 else 0\n","\n","def evaluate_with_msmarco(query, retrieved_docs, ground_truth):\n","    precision_k = precision_at_k(retrieved_docs, ground_truth, k=10)\n","    recall_k = recall_at_k(retrieved_docs, ground_truth, k=10)\n","    print(f\"ðŸ§ª MS MARCO Evaluation:\")\n","    print(f\"   - Precision@10: {precision_k:.2f}\")\n","    print(f\"   - Recall@10:    {recall_k:.2f}\")\n","\n","# Example with document IDs\n","query = \"What is the capital of France?\"\n","retrieved_docs = ['D1', 'D2', 'D5', 'D9', 'D3', 'D6', 'D7', 'D8', 'D10', 'D11']\n","ground_truth = {'D1', 'D3', 'D5'}  # MS MARCO provides these as QRELs\n","\n","evaluate_with_msmarco(query, retrieved_docs, ground_truth)\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1oPatwD18L73byPwZAyQHXM7DvMjmcuqG","timestamp":1751124638100}],"authorship_tag":"ABX9TyM0jWbeS7tFE4eLLm5RQlzA"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}